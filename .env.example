# AGNTCY Classification System - Environment Variables

# Gateway
GATEWAY_PORT=8080
MINIO_ENDPOINT=localhost:9010
MINIO_ACCESS_KEY=minioadmin
MINIO_SECRET_KEY=minioadmin
PLANNER_URL=http://localhost:8083

# ADS (Agent Directory Service)
ADS_PORT=8082
NATS_URL=nats://localhost:4222

# Planner
PLANNER_PORT=8083
ADS_URL=http://localhost:8082
SLIM_TIMEOUT_MS=10000

# Discovery Mode
# Options: "static" (hardcoded agents) or "ads" (dynamic discovery)
# Default: static (recommended for getting started)
DISCOVERY_MODE=static

# Planner Mode
# Options: "simple" (traditional Python class) or "langgraph" (stateful workflow)
# Default: simple (faster, less overhead)
# Use langgraph for: complex routing, conditional branching, state persistence
PLANNER_MODE=simple

# Agents
AGENT_PORT_MEDICAL=9001
AGENT_PORT_SATELLITE=9002
AGENT_PORT_GENERAL=9003

# Agent Heartbeat (for ADS registration)
# Set to "true" to enable, "false" to disable
# Default: false (agents work without ADS)
ENABLE_ADS_HEARTBEAT=false

# ============================================
# A2A Transport Configuration (Lungo-style)
# ============================================

# Transport type: "NATS" or "SLIM"
# - NATS: Message queue (default, async)
# - SLIM: HTTP-based protocol (for group conversations)
DEFAULT_MESSAGE_TRANSPORT=NATS

# Transport endpoint
# - For NATS: nats://localhost:4222
# - For SLIM: http://localhost:46357
TRANSPORT_SERVER_ENDPOINT=nats://localhost:4222

# Broadcast topic for all agents
FARM_BROADCAST_TOPIC=agents.broadcast

# Enable HTTP REST API on farm agents
ENABLE_HTTP=true

# Infrastructure
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin

# Observability (optional)
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4317
OTEL_SERVICE_NAME=classification-system

# Security (optional - requires Identity service)
IDENTITY_AUTH_ENABLED=false
IDENTITY_SERVICE_URL=http://localhost:8081

# ============================================
# LLM Configuration (Lungo-style with litellm)
# ============================================

# LLM Provider Settings
# Uses litellm to manage LLM connections uniformly across different providers
# Full list of supported providers: https://docs.litellm.ai/docs/providers
# Note: The environment variable for specifying the model is always LLM_MODEL, regardless of provider

# OpenAI (Recommended for development)
LLM_MODEL=openai/gpt-4o-mini
# OPENAI_API_KEY=your_openai_api_key_here

# Azure OpenAI
# LLM_MODEL=azure/your_deployment_name
# AZURE_API_KEY=your_azure_api_key
# AZURE_API_BASE=https://your-resource.openai.azure.com/
# AZURE_API_VERSION=2024-02-15-preview

# Anthropic Claude
# LLM_MODEL=anthropic/claude-3-5-sonnet-20241022
# ANTHROPIC_API_KEY=your_anthropic_api_key

# Groq (Fast inference)
# LLM_MODEL=groq/llama-3.3-70b-versatile
# GROQ_API_KEY=your_groq_api_key

# LiteLLM Proxy (For centralized LLM management)
# LLM_MODEL=azure/your_deployment_name
# LITELLM_PROXY_BASE_URL=http://your-litellm-proxy:4000
# LITELLM_PROXY_API_KEY=your_proxy_api_key

# Ollama (Local models)
# LLM_MODEL=ollama/llama3.2
# OLLAMA_API_BASE=http://localhost:11434

# NVIDIA NIM
# LLM_MODEL=nvidia_nim/meta/llama-3.1-8b-instruct
# NVIDIA_API_KEY=your_nvidia_api_key

# LLM Parameters
OPENAI_TEMPERATURE=0.7
LLM_MAX_TOKENS=4096
